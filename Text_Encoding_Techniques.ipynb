{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text dataset\n",
    "data_corpus = ['this product is very cheap and affordable',\n",
    "              'the Product is good',\n",
    "              'the product is bad',\n",
    "              'this product is very helpfull',\n",
    "              'hope you are doing well']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential libraries\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag Of Words Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python Implementation\n",
    "def fit(dataset):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if(isinstance(dataset, (list,))):\n",
    "        for row in dataset: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocab):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(dataset): # for each document in the dataset\n",
    "            \n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            # for every unique word in the document\n",
    "            for word, freq in word_freq.items():  # for each unique word in the review.                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                col_index = vocab.get(word, -1) \n",
    "                # if the word exists\n",
    "                if col_index !=-1:\n",
    "                    \n",
    "                    rows.append(idx)\n",
    "                   \n",
    "                    columns.append(col_index)\n",
    "                    \n",
    "                    values.append(freq)\n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product', 'affordable', 'and', 'are', 'bad', 'cheap', 'doing', 'good', 'helpfull', 'hope', 'is', 'product', 'the', 'this', 'very', 'well', 'you']\n",
      "[[0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = fit(data_corpus)\n",
    "print(list(vocab.keys()))\n",
    "print(transform(data_corpus, vocab).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0]\n",
      " [0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#sklearn implementation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(analyzer='word')\n",
    "\n",
    "vec.fit(data_corpus)\n",
    "feature_matrix = vec.transform(data_corpus)\n",
    "print(feature_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Tf-Idf Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    words = set()\n",
    "    if(isinstance(dataset,(list,))):\n",
    "        for i in dataset:\n",
    "            for w in i.split():\n",
    "                if(len(w)<2):\n",
    "                    continue\n",
    "                words.add(w)\n",
    "        words = sorted(list(words))\n",
    "        vocab = {i:j for j,i in enumerate(words)}\n",
    "        idf_vector = IDF(dataset,words)\n",
    "        return vocab,idf_vector\n",
    "    else:\n",
    "        print(\"Please pass list of sentences\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def IDF(dataset,words):\n",
    "    idf_val = {}\n",
    "    total_doc_len = len(dataset)\n",
    "    for i in words:\n",
    "        count = 0\n",
    "        for j in dataset:\n",
    "            if(i in j.split()):\n",
    "                count+=1\n",
    "        val = 1+(math.log((1+total_doc_len)/(1+count)))\n",
    "        idf_val[i]=val\n",
    "    return idf_val\n",
    "\n",
    "\n",
    "\n",
    "def transform(dataset,vocab,idf_vector):\n",
    "    rows=[]\n",
    "    columns = []\n",
    "    values = []\n",
    "    if(isinstance(dataset,(list))):\n",
    "        for i in range(len(dataset)):\n",
    "            length = len(dataset[i])\n",
    "            word_freq = dict(Counter(dataset[i].split()))\n",
    "            for word,word_length in word_freq.items():\n",
    "                tf_idf_val = (word_length/length)*idf_vector[word]\n",
    "                col_index = vocab.get(word,-1)\n",
    "                if(col_index!=-1):\n",
    "                    rows.append(i)\n",
    "                    columns.append(col_index)\n",
    "                    values.append(tf_idf_val)\n",
    "        sparse_matrix = csr_matrix((values,(rows,columns)),shape=(len(dataset),len(vocab)))\n",
    "        output_matrix = normalize(sparse_matrix,norm='l2')\n",
    "        return output_matrix\n",
    "    \n",
    "    else:\n",
    "        print(\"pass a list of sentences as dataset\")\n",
    "                    \n",
    "\n",
    "        \n",
    "vocab,idf_vector = fit(data_corpus)\n",
    "output_matrix = transform(data_corpus,vocab,idf_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x17 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom implementation feature names :\n",
      " ['Product', 'affordable', 'and', 'are', 'bad', 'cheap', 'doing', 'good', 'helpfull', 'hope', 'is', 'product', 'the', 'this', 'very', 'well', 'you']\n",
      "custom implemented tfidf matrix : \n",
      " [[0.         0.44421436 0.44421436 0.         0.         0.44421436\n",
      "  0.         0.         0.         0.         0.25026262 0.29749553\n",
      "  0.         0.35838935 0.35838935 0.         0.        ]\n",
      " [0.58042343 0.         0.         0.         0.         0.\n",
      "  0.         0.58042343 0.         0.         0.32700044 0.\n",
      "  0.46828197 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.64324583 0.\n",
      "  0.         0.         0.         0.         0.36239348 0.43078923\n",
      "  0.51896668 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5709398  0.         0.32165752 0.38236504\n",
      "  0.         0.46063063 0.46063063 0.         0.        ]\n",
      " [0.         0.         0.         0.4472136  0.         0.\n",
      "  0.4472136  0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.4472136  0.4472136 ]]\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "vocab,idf_vector = fit(data_corpus)\n",
    "output_matrix = transform(data_corpus,vocab,idf_vector)\n",
    "\n",
    "print(\"custom implementation feature names :\\n {}\".format(list(vocab.keys())))  \n",
    "print(\"custom implemented tfidf matrix : \\n {}\".format(output_matrix.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(data_corpus)\n",
    "skl_output = vectorizer.transform(data_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13)\t0.363117453789113\n",
      "  (0, 12)\t0.363117453789113\n",
      "  (0, 10)\t0.25356424898691843\n",
      "  (0, 9)\t0.25356424898691843\n",
      "  (0, 4)\t0.45007472445466307\n",
      "  (0, 1)\t0.45007472445466307\n",
      "  (0, 0)\t0.45007472445466307\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
